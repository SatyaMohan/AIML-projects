{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pp68FAQf9aMN"
   },
   "source": [
    "# Sarcasm Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bEahVPtWX5ve"
   },
   "source": [
    "### Dataset\n",
    "\n",
    "#### Acknowledgement\n",
    "Misra, Rishabh, and Prahal Arora. \"Sarcasm Detection using Hybrid Neural Network.\" arXiv preprint arXiv:1908.07414 (2019).\n",
    "\n",
    "**Required Files given in below link.**\n",
    "\n",
    "https://drive.google.com/drive/folders/1xUnF35naPGU63xwRDVGc-DkZ3M8V5mMk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow.keras as keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vAk6BRUh8CqL"
   },
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v8-PQsV0DrAZ"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_link</th>\n",
       "      <th>headline</th>\n",
       "      <th>is_sarcastic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/versace-b...</td>\n",
       "      <td>former versace store clerk sues over secret 'b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/roseanne-...</td>\n",
       "      <td>the 'roseanne' revival catches up to our thorn...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://local.theonion.com/mom-starting-to-fea...</td>\n",
       "      <td>mom starting to fear son's web series closest ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://politics.theonion.com/boehner-just-wan...</td>\n",
       "      <td>boehner just wants wife to listen, not come up...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/jk-rowlin...</td>\n",
       "      <td>j.k. rowling wishes snape happy birthday in th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        article_link  \\\n",
       "0  https://www.huffingtonpost.com/entry/versace-b...   \n",
       "1  https://www.huffingtonpost.com/entry/roseanne-...   \n",
       "2  https://local.theonion.com/mom-starting-to-fea...   \n",
       "3  https://politics.theonion.com/boehner-just-wan...   \n",
       "4  https://www.huffingtonpost.com/entry/jk-rowlin...   \n",
       "\n",
       "                                            headline  is_sarcastic  \n",
       "0  former versace store clerk sues over secret 'b...             0  \n",
       "1  the 'roseanne' revival catches up to our thorn...             0  \n",
       "2  mom starting to fear son's web series closest ...             1  \n",
       "3  boehner just wants wife to listen, not come up...             1  \n",
       "4  j.k. rowling wishes snape happy birthday in th...             0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_json('Sarcasm_Headlines_Dataset.json', lines=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z6pXf7A78E2H"
   },
   "source": [
    "### Drop `article_link` from dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-WUNHq5zEV0n"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "      <th>is_sarcastic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>former versace store clerk sues over secret 'b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the 'roseanne' revival catches up to our thorn...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mom starting to fear son's web series closest ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>boehner just wants wife to listen, not come up...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>j.k. rowling wishes snape happy birthday in th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            headline  is_sarcastic\n",
       "0  former versace store clerk sues over secret 'b...             0\n",
       "1  the 'roseanne' revival catches up to our thorn...             0\n",
       "2  mom starting to fear son's web series closest ...             1\n",
       "3  boehner just wants wife to listen, not come up...             1\n",
       "4  j.k. rowling wishes snape happy birthday in th...             0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.drop('article_link', axis=1, inplace=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D0h6IOxU8OdH"
   },
   "source": [
    "### Get length of each headline and add a column for that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MLpiBRDmEV2l"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "      <th>is_sarcastic</th>\n",
       "      <th>num_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>former versace store clerk sues over secret 'b...</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the 'roseanne' revival catches up to our thorn...</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mom starting to fear son's web series closest ...</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>boehner just wants wife to listen, not come up...</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>j.k. rowling wishes snape happy birthday in th...</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            headline  is_sarcastic  num_words\n",
       "0  former versace store clerk sues over secret 'b...             0         12\n",
       "1  the 'roseanne' revival catches up to our thorn...             0         14\n",
       "2  mom starting to fear son's web series closest ...             1         14\n",
       "3  boehner just wants wife to listen, not come up...             1         13\n",
       "4  j.k. rowling wishes snape happy birthday in th...             0         11"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get number of words in each headline and add it as a column\n",
    "data['num_words'] = data['headline'].apply(lambda x: len(x.split()))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = data['headline']\n",
    "y = data['is_sarcastic']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "# Convert to numpy arrays\n",
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SMF-wjJ2aMwm"
   },
   "source": [
    "### Initialize parameter values\n",
    "- Set values for max_features, maxlen, & embedding_size\n",
    "- max_features: Number of words to take from tokenizer(most frequent words)\n",
    "- maxlen: Maximum length of each sentence to be limited to 25\n",
    "- embedding_size: size of embedding vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jPw9gAN_EV6m"
   },
   "outputs": [],
   "source": [
    "max_features = 10000\n",
    "maxlen = 25\n",
    "embedding_size = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9abSe-bM8fn9"
   },
   "source": [
    "### Apply `tensorflow.keras` Tokenizer and get indices for words\n",
    "- Initialize Tokenizer object with number of words as 10000\n",
    "- Fit the tokenizer object on headline column\n",
    "- Convert the text to sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y8g4l0KfF3eh"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "\n",
    "# Fit on train texts and convert them to sequences \n",
    "tokenizer.fit_on_texts(X_train) # X_train is np.array having training headlines\n",
    "X_train_sequences = tokenizer.texts_to_sequences(X_train)\n",
    "\n",
    "# Convert the test texts to sequences\n",
    "X_test_sequences = tokenizer.texts_to_sequences(X_test) # X_test is np.array having test headlines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xeZpwPO4bOkZ"
   },
   "source": [
    "### Pad sequences\n",
    "- Pad each example with a maximum length\n",
    "- Convert target column into numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qV0K70E5c9Xl"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Pad the sentences with 0 to mark them as unknown word\n",
    "X_train_sequences_pad = pad_sequences(X_train_sequences, maxlen=maxlen, padding='post', value=0)\n",
    "X_test_sequences_pad = pad_sequences(X_test_sequences, maxlen=maxlen, padding='post', value=0)\n",
    "\n",
    "# Convert targets to numpy array\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WJLyKg-98rH_"
   },
   "source": [
    "### Vocab mapping\n",
    "- There is no word for 0th index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vCNgtnQqdbZn"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'to': 1,\n",
       " 'of': 2,\n",
       " 'the': 3,\n",
       " 'in': 4,\n",
       " 'for': 5,\n",
       " 'a': 6,\n",
       " 'on': 7,\n",
       " 'and': 8,\n",
       " 'with': 9,\n",
       " 'is': 10,\n",
       " 'new': 11,\n",
       " 'trump': 12,\n",
       " 'man': 13,\n",
       " 'at': 14,\n",
       " 'from': 15,\n",
       " 'about': 16,\n",
       " 'you': 17,\n",
       " 'by': 18,\n",
       " 'this': 19,\n",
       " 'after': 20,\n",
       " 'up': 21,\n",
       " 'be': 22,\n",
       " 'out': 23,\n",
       " 'how': 24,\n",
       " 'it': 25,\n",
       " 'that': 26,\n",
       " 'as': 27,\n",
       " 'not': 28,\n",
       " 'are': 29,\n",
       " 'your': 30,\n",
       " 'his': 31,\n",
       " 'what': 32,\n",
       " 'he': 33,\n",
       " 'all': 34,\n",
       " 'just': 35,\n",
       " 'has': 36,\n",
       " 'who': 37,\n",
       " 'into': 38,\n",
       " 'one': 39,\n",
       " 'more': 40,\n",
       " 'report': 41,\n",
       " 'will': 42,\n",
       " 'why': 43,\n",
       " 'year': 44,\n",
       " 'over': 45,\n",
       " 'area': 46,\n",
       " 'have': 47,\n",
       " 'day': 48,\n",
       " 'says': 49,\n",
       " 'u': 50,\n",
       " 'can': 51,\n",
       " 'donald': 52,\n",
       " 's': 53,\n",
       " 'woman': 54,\n",
       " 'first': 55,\n",
       " 'time': 56,\n",
       " 'no': 57,\n",
       " 'get': 58,\n",
       " 'like': 59,\n",
       " 'old': 60,\n",
       " 'off': 61,\n",
       " \"trump's\": 62,\n",
       " 'her': 63,\n",
       " 'obama': 64,\n",
       " 'an': 65,\n",
       " 'now': 66,\n",
       " 'life': 67,\n",
       " 'people': 68,\n",
       " \"'\": 69,\n",
       " 'was': 70,\n",
       " 'make': 71,\n",
       " 'women': 72,\n",
       " 'house': 73,\n",
       " 'than': 74,\n",
       " 'still': 75,\n",
       " 'white': 76,\n",
       " 'back': 77,\n",
       " 'my': 78,\n",
       " 'if': 79,\n",
       " 'clinton': 80,\n",
       " 'when': 81,\n",
       " '5': 82,\n",
       " 'i': 83,\n",
       " 'down': 84,\n",
       " 'we': 85,\n",
       " 'their': 86,\n",
       " 'world': 87,\n",
       " 'could': 88,\n",
       " 'do': 89,\n",
       " 'family': 90,\n",
       " 'americans': 91,\n",
       " 'way': 92,\n",
       " 'before': 93,\n",
       " 'most': 94,\n",
       " 'police': 95,\n",
       " \"it's\": 96,\n",
       " 'gop': 97,\n",
       " 'bill': 98,\n",
       " 'school': 99,\n",
       " 'best': 100,\n",
       " 'study': 101,\n",
       " 'should': 102,\n",
       " 'american': 103,\n",
       " 'black': 104,\n",
       " 'being': 105,\n",
       " 'only': 106,\n",
       " '3': 107,\n",
       " 'years': 108,\n",
       " 'they': 109,\n",
       " 'him': 110,\n",
       " 'would': 111,\n",
       " 'so': 112,\n",
       " 'really': 113,\n",
       " 'watch': 114,\n",
       " 'last': 115,\n",
       " 'or': 116,\n",
       " 'during': 117,\n",
       " 'state': 118,\n",
       " 'death': 119,\n",
       " 'know': 120,\n",
       " '10': 121,\n",
       " 'home': 122,\n",
       " 'president': 123,\n",
       " 'show': 124,\n",
       " 'health': 125,\n",
       " 'say': 126,\n",
       " \"can't\": 127,\n",
       " 'video': 128,\n",
       " 'finds': 129,\n",
       " 'things': 130,\n",
       " 'but': 131,\n",
       " 'hillary': 132,\n",
       " 'going': 133,\n",
       " 'every': 134,\n",
       " \"'the\": 135,\n",
       " 'may': 136,\n",
       " 'campaign': 137,\n",
       " 'mom': 138,\n",
       " 'good': 139,\n",
       " '2': 140,\n",
       " 'nation': 141,\n",
       " 'party': 142,\n",
       " 'right': 143,\n",
       " 'getting': 144,\n",
       " 'some': 145,\n",
       " 'she': 146,\n",
       " 'change': 147,\n",
       " 'gets': 148,\n",
       " 'kids': 149,\n",
       " 'love': 150,\n",
       " 'against': 151,\n",
       " 'big': 152,\n",
       " 'other': 153,\n",
       " '000': 154,\n",
       " 'these': 155,\n",
       " 'makes': 156,\n",
       " 'need': 157,\n",
       " 'too': 158,\n",
       " 'take': 159,\n",
       " 'court': 160,\n",
       " 'little': 161,\n",
       " 'high': 162,\n",
       " 'self': 163,\n",
       " 'dead': 164,\n",
       " 'work': 165,\n",
       " 'parents': 166,\n",
       " 'gay': 167,\n",
       " \"doesn't\": 168,\n",
       " 'john': 169,\n",
       " 'never': 170,\n",
       " 'where': 171,\n",
       " 'our': 172,\n",
       " '4': 173,\n",
       " 'through': 174,\n",
       " 'election': 175,\n",
       " 'look': 176,\n",
       " '7': 177,\n",
       " 'calls': 178,\n",
       " 'see': 179,\n",
       " 'while': 180,\n",
       " 'news': 181,\n",
       " 'want': 182,\n",
       " 'own': 183,\n",
       " \"don't\": 184,\n",
       " 'real': 185,\n",
       " \"here's\": 186,\n",
       " 'child': 187,\n",
       " 'sex': 188,\n",
       " 'takes': 189,\n",
       " 'stop': 190,\n",
       " 'its': 191,\n",
       " 'local': 192,\n",
       " \"he's\": 193,\n",
       " 'next': 194,\n",
       " \"nation's\": 195,\n",
       " 'go': 196,\n",
       " 'plan': 197,\n",
       " 'even': 198,\n",
       " 'america': 199,\n",
       " 'million': 200,\n",
       " 'baby': 201,\n",
       " 'again': 202,\n",
       " 'college': 203,\n",
       " 'bush': 204,\n",
       " 'gun': 205,\n",
       " 'another': 206,\n",
       " 'war': 207,\n",
       " 'dad': 208,\n",
       " 'guy': 209,\n",
       " 'week': 210,\n",
       " 'office': 211,\n",
       " 'two': 212,\n",
       " 'dog': 213,\n",
       " '1': 214,\n",
       " '6': 215,\n",
       " 'wants': 216,\n",
       " 'made': 217,\n",
       " 'thing': 218,\n",
       " 'care': 219,\n",
       " 'got': 220,\n",
       " 'been': 221,\n",
       " 'debate': 222,\n",
       " 'ever': 223,\n",
       " 'us': 224,\n",
       " 'them': 225,\n",
       " 'help': 226,\n",
       " \"won't\": 227,\n",
       " 'ways': 228,\n",
       " 'money': 229,\n",
       " \"man's\": 230,\n",
       " 'season': 231,\n",
       " 'star': 232,\n",
       " 'national': 233,\n",
       " 'climate': 234,\n",
       " 'long': 235,\n",
       " 'under': 236,\n",
       " 'sexual': 237,\n",
       " 'job': 238,\n",
       " 'congress': 239,\n",
       " '20': 240,\n",
       " 'live': 241,\n",
       " 'night': 242,\n",
       " 'north': 243,\n",
       " 'media': 244,\n",
       " 'god': 245,\n",
       " 'much': 246,\n",
       " 'history': 247,\n",
       " 'finally': 248,\n",
       " 'shooting': 249,\n",
       " '8': 250,\n",
       " 'paul': 251,\n",
       " 'top': 252,\n",
       " 'around': 253,\n",
       " 'couple': 254,\n",
       " 'anti': 255,\n",
       " '9': 256,\n",
       " 'announces': 257,\n",
       " 'introduces': 258,\n",
       " 'teen': 259,\n",
       " 'making': 260,\n",
       " 'better': 261,\n",
       " 'there': 262,\n",
       " 'reveals': 263,\n",
       " 'any': 264,\n",
       " 'game': 265,\n",
       " 'law': 266,\n",
       " 'food': 267,\n",
       " 'shows': 268,\n",
       " 'bad': 269,\n",
       " 'without': 270,\n",
       " 'trying': 271,\n",
       " 'supreme': 272,\n",
       " 'tv': 273,\n",
       " 'story': 274,\n",
       " 'had': 275,\n",
       " 'senate': 276,\n",
       " 'great': 277,\n",
       " 'end': 278,\n",
       " 'students': 279,\n",
       " 'free': 280,\n",
       " 'actually': 281,\n",
       " 'tell': 282,\n",
       " 'give': 283,\n",
       " 'face': 284,\n",
       " 'film': 285,\n",
       " 'away': 286,\n",
       " 'fight': 287,\n",
       " 'enough': 288,\n",
       " 'everyone': 289,\n",
       " 'business': 290,\n",
       " 'government': 291,\n",
       " 'friend': 292,\n",
       " 'wedding': 293,\n",
       " 'pope': 294,\n",
       " 'son': 295,\n",
       " 'entire': 296,\n",
       " 'body': 297,\n",
       " 'part': 298,\n",
       " 'second': 299,\n",
       " 'deal': 300,\n",
       " 'call': 301,\n",
       " 'book': 302,\n",
       " 'republican': 303,\n",
       " 'city': 304,\n",
       " 'me': 305,\n",
       " 'movie': 306,\n",
       " 'facebook': 307,\n",
       " 'line': 308,\n",
       " 'attack': 309,\n",
       " 'men': 310,\n",
       " 'middle': 311,\n",
       " 'does': 312,\n",
       " 'children': 313,\n",
       " 'found': 314,\n",
       " 'york': 315,\n",
       " 'friends': 316,\n",
       " 'speech': 317,\n",
       " 'single': 318,\n",
       " 'christmas': 319,\n",
       " 'public': 320,\n",
       " 'come': 321,\n",
       " 'fire': 322,\n",
       " '11': 323,\n",
       " 'think': 324,\n",
       " 'find': 325,\n",
       " 'photos': 326,\n",
       " 'behind': 327,\n",
       " 'girl': 328,\n",
       " 'must': 329,\n",
       " 'keep': 330,\n",
       " 'rights': 331,\n",
       " 'talk': 332,\n",
       " 'thinks': 333,\n",
       " 'goes': 334,\n",
       " 'support': 335,\n",
       " 'former': 336,\n",
       " 'looking': 337,\n",
       " 'same': 338,\n",
       " 'car': 339,\n",
       " 'morning': 340,\n",
       " 'democrats': 341,\n",
       " 'used': 342,\n",
       " 'tax': 343,\n",
       " 'power': 344,\n",
       " 'sanders': 345,\n",
       " 'future': 346,\n",
       " 'presidential': 347,\n",
       " 'republicans': 348,\n",
       " 'open': 349,\n",
       " 'james': 350,\n",
       " \"world's\": 351,\n",
       " 'name': 352,\n",
       " 'once': 353,\n",
       " 'run': 354,\n",
       " 'violence': 355,\n",
       " 'doing': 356,\n",
       " 'marriage': 357,\n",
       " 'control': 358,\n",
       " 'vote': 359,\n",
       " 'use': 360,\n",
       " 'security': 361,\n",
       " 'room': 362,\n",
       " 'team': 363,\n",
       " 'between': 364,\n",
       " 'group': 365,\n",
       " 'win': 366,\n",
       " 'email': 367,\n",
       " 'asks': 368,\n",
       " 'pretty': 369,\n",
       " 'each': 370,\n",
       " 'music': 371,\n",
       " 'case': 372,\n",
       " 'full': 373,\n",
       " 'having': 374,\n",
       " 'secret': 375,\n",
       " 'something': 376,\n",
       " 'female': 377,\n",
       " 'company': 378,\n",
       " 'human': 379,\n",
       " 'coming': 380,\n",
       " 'three': 381,\n",
       " \"didn't\": 382,\n",
       " 'already': 383,\n",
       " 'sure': 384,\n",
       " 'bernie': 385,\n",
       " 'inside': 386,\n",
       " 'student': 387,\n",
       " 'ad': 388,\n",
       " 'killed': 389,\n",
       " 'might': 390,\n",
       " 'releases': 391,\n",
       " '2016': 392,\n",
       " 'poll': 393,\n",
       " 'ryan': 394,\n",
       " 'department': 395,\n",
       " 'person': 396,\n",
       " 'scientists': 397,\n",
       " 'photo': 398,\n",
       " 'forced': 399,\n",
       " 'meet': 400,\n",
       " '12': 401,\n",
       " 'tells': 402,\n",
       " 'voters': 403,\n",
       " 'race': 404,\n",
       " 'gives': 405,\n",
       " 'ban': 406,\n",
       " 'post': 407,\n",
       " 'claims': 408,\n",
       " 'living': 409,\n",
       " 'fans': 410,\n",
       " 'missing': 411,\n",
       " 'boy': 412,\n",
       " 'very': 413,\n",
       " 'because': 414,\n",
       " 'father': 415,\n",
       " 'perfect': 416,\n",
       " 'pay': 417,\n",
       " 'here': 418,\n",
       " 'california': 419,\n",
       " 'everything': 420,\n",
       " 'summer': 421,\n",
       " \"you're\": 422,\n",
       " 'reports': 423,\n",
       " 'looks': 424,\n",
       " 'admits': 425,\n",
       " 'needs': 426,\n",
       " 'twitter': 427,\n",
       " 'texas': 428,\n",
       " 'plans': 429,\n",
       " '15': 430,\n",
       " 'hot': 431,\n",
       " 'running': 432,\n",
       " 'wife': 433,\n",
       " 'comes': 434,\n",
       " 'states': 435,\n",
       " 'ready': 436,\n",
       " 'wall': 437,\n",
       " 'always': 438,\n",
       " 'phone': 439,\n",
       " 'age': 440,\n",
       " 'art': 441,\n",
       " 'service': 442,\n",
       " 'many': 443,\n",
       " 'justice': 444,\n",
       " 'dies': 445,\n",
       " 'water': 446,\n",
       " 'until': 447,\n",
       " 'list': 448,\n",
       " 'political': 449,\n",
       " 'judge': 450,\n",
       " 'red': 451,\n",
       " 'candidate': 452,\n",
       " 'save': 453,\n",
       " 'social': 454,\n",
       " 'secretary': 455,\n",
       " 'country': 456,\n",
       " \"women's\": 457,\n",
       " 'class': 458,\n",
       " 'working': 459,\n",
       " 'teacher': 460,\n",
       " 'talks': 461,\n",
       " 'unveils': 462,\n",
       " 'head': 463,\n",
       " 'employee': 464,\n",
       " 'probably': 465,\n",
       " 'ceo': 466,\n",
       " 'days': 467,\n",
       " 'kim': 468,\n",
       " 'today': 469,\n",
       " 'eating': 470,\n",
       " 'someone': 471,\n",
       " 'super': 472,\n",
       " 'minutes': 473,\n",
       " 'hours': 474,\n",
       " 'idea': 475,\n",
       " 'heart': 476,\n",
       " \"'i\": 477,\n",
       " 'russia': 478,\n",
       " 'were': 479,\n",
       " 'times': 480,\n",
       " 'lives': 481,\n",
       " 'mother': 482,\n",
       " 'ex': 483,\n",
       " 'start': 484,\n",
       " 'cancer': 485,\n",
       " 'town': 486,\n",
       " 'chief': 487,\n",
       " 'thousands': 488,\n",
       " 'did': 489,\n",
       " '30': 490,\n",
       " 'percent': 491,\n",
       " 'outside': 492,\n",
       " 'put': 493,\n",
       " \"i'm\": 494,\n",
       " 'young': 495,\n",
       " 'george': 496,\n",
       " 'nuclear': 497,\n",
       " 'feel': 498,\n",
       " 'lost': 499,\n",
       " 'taking': 500,\n",
       " 'muslim': 501,\n",
       " 'drug': 502,\n",
       " 'hard': 503,\n",
       " 'cat': 504,\n",
       " 'michael': 505,\n",
       " 'past': 506,\n",
       " 'reason': 507,\n",
       " 'restaurant': 508,\n",
       " 'air': 509,\n",
       " 'administration': 510,\n",
       " 'set': 511,\n",
       " 'obamacare': 512,\n",
       " 'dream': 513,\n",
       " 'tips': 514,\n",
       " 'record': 515,\n",
       " 'shot': 516,\n",
       " 'left': 517,\n",
       " 'rock': 518,\n",
       " 'director': 519,\n",
       " 'few': 520,\n",
       " 'internet': 521,\n",
       " 'fucking': 522,\n",
       " 'warns': 523,\n",
       " 'meeting': 524,\n",
       " '50': 525,\n",
       " 'series': 526,\n",
       " 'cruz': 527,\n",
       " 'month': 528,\n",
       " 'chris': 529,\n",
       " 'minute': 530,\n",
       " 'latest': 531,\n",
       " 'wins': 532,\n",
       " 'thought': 533,\n",
       " 'together': 534,\n",
       " 'daughter': 535,\n",
       " 'officials': 536,\n",
       " 'biden': 537,\n",
       " 'wrong': 538,\n",
       " 'months': 539,\n",
       " 'abortion': 540,\n",
       " 'third': 541,\n",
       " 'guide': 542,\n",
       " 'romney': 543,\n",
       " 'isis': 544,\n",
       " 'letter': 545,\n",
       " 'breaking': 546,\n",
       " 'mike': 547,\n",
       " 'let': 548,\n",
       " 'giving': 549,\n",
       " 'favorite': 550,\n",
       " 'majority': 551,\n",
       " 'korea': 552,\n",
       " 'holiday': 553,\n",
       " 'kill': 554,\n",
       " 'attacks': 555,\n",
       " 'south': 556,\n",
       " 'stars': 557,\n",
       " 'crisis': 558,\n",
       " 'less': 559,\n",
       " 'congressman': 560,\n",
       " 'place': 561,\n",
       " 'education': 562,\n",
       " '2015': 563,\n",
       " 'word': 564,\n",
       " 'street': 565,\n",
       " 'system': 566,\n",
       " 'online': 567,\n",
       " 'leaves': 568,\n",
       " \"america's\": 569,\n",
       " 'lot': 570,\n",
       " 'king': 571,\n",
       " 'half': 572,\n",
       " 'relationship': 573,\n",
       " 'small': 574,\n",
       " 'hit': 575,\n",
       " 'scott': 576,\n",
       " 'problem': 577,\n",
       " 'community': 578,\n",
       " 'questions': 579,\n",
       " 'ice': 580,\n",
       " 'beautiful': 581,\n",
       " 'wearing': 582,\n",
       " 'kid': 583,\n",
       " 'play': 584,\n",
       " 'thinking': 585,\n",
       " 'tweets': 586,\n",
       " 'march': 587,\n",
       " 'believe': 588,\n",
       " 'iran': 589,\n",
       " 'those': 590,\n",
       " 'yet': 591,\n",
       " 'talking': 592,\n",
       " \"what's\": 593,\n",
       " 'al': 594,\n",
       " 'federal': 595,\n",
       " 'move': 596,\n",
       " 'message': 597,\n",
       " 'owner': 598,\n",
       " 'fbi': 599,\n",
       " 'ted': 600,\n",
       " 'himself': 601,\n",
       " 'senator': 602,\n",
       " 'fan': 603,\n",
       " 'interview': 604,\n",
       " 'hollywood': 605,\n",
       " 'immigration': 606,\n",
       " 'sleep': 607,\n",
       " 'leave': 608,\n",
       " 'different': 609,\n",
       " 'non': 610,\n",
       " 'using': 611,\n",
       " 'washington': 612,\n",
       " 'well': 613,\n",
       " 'knows': 614,\n",
       " 't': 615,\n",
       " 'happy': 616,\n",
       " 'response': 617,\n",
       " \"she's\": 618,\n",
       " 'issues': 619,\n",
       " 'excited': 620,\n",
       " 'gift': 621,\n",
       " 'shit': 622,\n",
       " \"obama's\": 623,\n",
       " 'birth': 624,\n",
       " 'box': 625,\n",
       " 'prison': 626,\n",
       " 'rise': 627,\n",
       " 'hope': 628,\n",
       " 'whole': 629,\n",
       " 'protest': 630,\n",
       " 'nothing': 631,\n",
       " 'buy': 632,\n",
       " 'millions': 633,\n",
       " 'order': 634,\n",
       " \"isn't\": 635,\n",
       " 'vows': 636,\n",
       " 'francis': 637,\n",
       " 'become': 638,\n",
       " 'democratic': 639,\n",
       " 'taylor': 640,\n",
       " 'birthday': 641,\n",
       " 'weekend': 642,\n",
       " 'personal': 643,\n",
       " 'assault': 644,\n",
       " 'union': 645,\n",
       " 'ask': 646,\n",
       " 'military': 647,\n",
       " 'earth': 648,\n",
       " 'victims': 649,\n",
       " 'florida': 650,\n",
       " 'travel': 651,\n",
       " 'puts': 652,\n",
       " 'host': 653,\n",
       " 'since': 654,\n",
       " 'cover': 655,\n",
       " 'celebrates': 656,\n",
       " 'huge': 657,\n",
       " 'russian': 658,\n",
       " 'following': 659,\n",
       " 'candidates': 660,\n",
       " 'hour': 661,\n",
       " 'leaders': 662,\n",
       " 'trip': 663,\n",
       " 'career': 664,\n",
       " 'watching': 665,\n",
       " 'least': 666,\n",
       " 'date': 667,\n",
       " '100': 668,\n",
       " 'bar': 669,\n",
       " 'mark': 670,\n",
       " 'girls': 671,\n",
       " 'stephen': 672,\n",
       " 'fun': 673,\n",
       " 'rules': 674,\n",
       " 'reasons': 675,\n",
       " 'kills': 676,\n",
       " 'david': 677,\n",
       " 'killing': 678,\n",
       " 'conversation': 679,\n",
       " 'words': 680,\n",
       " 'almost': 681,\n",
       " 'read': 682,\n",
       " 'fox': 683,\n",
       " 'moment': 684,\n",
       " 'bring': 685,\n",
       " 'sports': 686,\n",
       " 'investigation': 687,\n",
       " '40': 688,\n",
       " 'murder': 689,\n",
       " 'told': 690,\n",
       " 'front': 691,\n",
       " 'k': 692,\n",
       " 'called': 693,\n",
       " 'jimmy': 694,\n",
       " 'trailer': 695,\n",
       " 'leader': 696,\n",
       " 'cops': 697,\n",
       " 'politics': 698,\n",
       " 'signs': 699,\n",
       " 'tom': 700,\n",
       " 'awards': 701,\n",
       " 'powerful': 702,\n",
       " 'c': 703,\n",
       " 'key': 704,\n",
       " 'girlfriend': 705,\n",
       " 'policy': 706,\n",
       " 'adorable': 707,\n",
       " \"there's\": 708,\n",
       " 'cop': 709,\n",
       " 'trans': 710,\n",
       " 'early': 711,\n",
       " 'j': 712,\n",
       " 'store': 713,\n",
       " 'accused': 714,\n",
       " 'special': 715,\n",
       " 'weird': 716,\n",
       " 'act': 717,\n",
       " 'break': 718,\n",
       " 'lgbt': 719,\n",
       " 'hits': 720,\n",
       " 'driving': 721,\n",
       " '2017': 722,\n",
       " 'reality': 723,\n",
       " 'mass': 724,\n",
       " 'apple': 725,\n",
       " 'totally': 726,\n",
       " 'hate': 727,\n",
       " 'syrian': 728,\n",
       " 'waiting': 729,\n",
       " 'queer': 730,\n",
       " 'straight': 731,\n",
       " 'five': 732,\n",
       " 'worried': 733,\n",
       " '2014': 734,\n",
       " 'united': 735,\n",
       " 'lessons': 736,\n",
       " 'hands': 737,\n",
       " 'd': 738,\n",
       " 'seen': 739,\n",
       " 'return': 740,\n",
       " 'anniversary': 741,\n",
       " 'late': 742,\n",
       " 'billion': 743,\n",
       " 'visit': 744,\n",
       " 'huffpost': 745,\n",
       " 'feels': 746,\n",
       " 'schools': 747,\n",
       " 'iraq': 748,\n",
       " 'near': 749,\n",
       " 'advice': 750,\n",
       " 'syria': 751,\n",
       " 'opens': 752,\n",
       " 'global': 753,\n",
       " 'number': 754,\n",
       " 'sign': 755,\n",
       " 'spends': 756,\n",
       " 'low': 757,\n",
       " 'defense': 758,\n",
       " 'center': 759,\n",
       " 'coffee': 760,\n",
       " 'final': 761,\n",
       " 'offers': 762,\n",
       " 'die': 763,\n",
       " 'joe': 764,\n",
       " 'hair': 765,\n",
       " 'kind': 766,\n",
       " 'longer': 767,\n",
       " 'anything': 768,\n",
       " 'employees': 769,\n",
       " 'protesters': 770,\n",
       " 'planned': 771,\n",
       " 'abuse': 772,\n",
       " 'dance': 773,\n",
       " 'point': 774,\n",
       " 'data': 775,\n",
       " 'brings': 776,\n",
       " \"they're\": 777,\n",
       " 'fall': 778,\n",
       " 'un': 779,\n",
       " 'mind': 780,\n",
       " 'iowa': 781,\n",
       " 'reportedly': 782,\n",
       " 'fashion': 783,\n",
       " 'check': 784,\n",
       " 'returns': 785,\n",
       " 'stand': 786,\n",
       " 'dinner': 787,\n",
       " 'nfl': 788,\n",
       " 'surprise': 789,\n",
       " 'drunk': 790,\n",
       " 'oil': 791,\n",
       " 'worth': 792,\n",
       " 'pence': 793,\n",
       " 'hilarious': 794,\n",
       " 'completely': 795,\n",
       " 'true': 796,\n",
       " 'chicago': 797,\n",
       " 'lead': 798,\n",
       " 'experience': 799,\n",
       " 'coworker': 800,\n",
       " 'turns': 801,\n",
       " 'chinese': 802,\n",
       " 'biggest': 803,\n",
       " 'moving': 804,\n",
       " 'bus': 805,\n",
       " 'learned': 806,\n",
       " 'moore': 807,\n",
       " 'light': 808,\n",
       " 'starting': 809,\n",
       " 'williams': 810,\n",
       " 'style': 811,\n",
       " 'dating': 812,\n",
       " 'adds': 813,\n",
       " 'decision': 814,\n",
       " 'far': 815,\n",
       " 'prince': 816,\n",
       " 'stage': 817,\n",
       " 'band': 818,\n",
       " 'breaks': 819,\n",
       " 'supporters': 820,\n",
       " 'hoping': 821,\n",
       " 'discover': 822,\n",
       " 'struggling': 823,\n",
       " 'wait': 824,\n",
       " 'turn': 825,\n",
       " 'transgender': 826,\n",
       " 'worst': 827,\n",
       " 'space': 828,\n",
       " 'side': 829,\n",
       " 'oscars': 830,\n",
       " 'given': 831,\n",
       " 'across': 832,\n",
       " 'industry': 833,\n",
       " 'success': 834,\n",
       " 'risk': 835,\n",
       " 'carolina': 836,\n",
       " 'sick': 837,\n",
       " 'card': 838,\n",
       " 'china': 839,\n",
       " 'chance': 840,\n",
       " 'dying': 841,\n",
       " '13': 842,\n",
       " 'cut': 843,\n",
       " 'kardashian': 844,\n",
       " 'playing': 845,\n",
       " 'west': 846,\n",
       " 'press': 847,\n",
       " 'urges': 848,\n",
       " 'steve': 849,\n",
       " 'road': 850,\n",
       " 'chicken': 851,\n",
       " 'homeless': 852,\n",
       " 'suspect': 853,\n",
       " 'test': 854,\n",
       " 'apartment': 855,\n",
       " 'board': 856,\n",
       " 'shares': 857,\n",
       " 'hurricane': 858,\n",
       " 'massive': 859,\n",
       " 'loses': 860,\n",
       " 'artist': 861,\n",
       " 'university': 862,\n",
       " 'executive': 863,\n",
       " 'paris': 864,\n",
       " 'green': 865,\n",
       " 'ebola': 866,\n",
       " 'officer': 867,\n",
       " 'nyc': 868,\n",
       " 'fails': 869,\n",
       " 'close': 870,\n",
       " 'keeps': 871,\n",
       " 'demands': 872,\n",
       " 'hand': 873,\n",
       " 'epa': 874,\n",
       " 'allegations': 875,\n",
       " 'rubio': 876,\n",
       " 'opening': 877,\n",
       " 'football': 878,\n",
       " 'culture': 879,\n",
       " 'harassment': 880,\n",
       " 'battle': 881,\n",
       " 'hopes': 882,\n",
       " 'sean': 883,\n",
       " 'shop': 884,\n",
       " 'suicide': 885,\n",
       " 'magazine': 886,\n",
       " 'address': 887,\n",
       " 'governor': 888,\n",
       " 'question': 889,\n",
       " 'jobs': 890,\n",
       " 'israel': 891,\n",
       " 'major': 892,\n",
       " 'explains': 893,\n",
       " 'hear': 894,\n",
       " 'audience': 895,\n",
       " 'easy': 896,\n",
       " 'cool': 897,\n",
       " 'workers': 898,\n",
       " 'mental': 899,\n",
       " 'beauty': 900,\n",
       " 'amazing': 901,\n",
       " 'uses': 902,\n",
       " 'reminds': 903,\n",
       " 'push': 904,\n",
       " 'robert': 905,\n",
       " 'role': 906,\n",
       " 'netflix': 907,\n",
       " 'remember': 908,\n",
       " 'jr': 909,\n",
       " 'eat': 910,\n",
       " 'google': 911,\n",
       " 'voice': 912,\n",
       " 'halloween': 913,\n",
       " 'hall': 914,\n",
       " 'four': 915,\n",
       " 'fear': 916,\n",
       " 'crash': 917,\n",
       " 'possible': 918,\n",
       " 'picture': 919,\n",
       " 'simple': 920,\n",
       " 'amid': 921,\n",
       " 'fighting': 922,\n",
       " 'pizza': 923,\n",
       " 'try': 924,\n",
       " 'grandma': 925,\n",
       " 'skin': 926,\n",
       " 'oscar': 927,\n",
       " 'foreign': 928,\n",
       " 'general': 929,\n",
       " 'ferguson': 930,\n",
       " 'names': 931,\n",
       " 'economy': 932,\n",
       " 'program': 933,\n",
       " 'toward': 934,\n",
       " 'throws': 935,\n",
       " 'users': 936,\n",
       " 'residents': 937,\n",
       " 'colbert': 938,\n",
       " 'incredible': 939,\n",
       " 'results': 940,\n",
       " 'sales': 941,\n",
       " 'important': 942,\n",
       " 'voter': 943,\n",
       " 'ideas': 944,\n",
       " 'steps': 945,\n",
       " 'spot': 946,\n",
       " 'passes': 947,\n",
       " 'reform': 948,\n",
       " 'six': 949,\n",
       " 'deadly': 950,\n",
       " 'official': 951,\n",
       " '17': 952,\n",
       " 'amazon': 953,\n",
       " 'responds': 954,\n",
       " 'clearly': 955,\n",
       " 'plane': 956,\n",
       " 'kerry': 957,\n",
       " 'apologizes': 958,\n",
       " 'suggests': 959,\n",
       " '18': 960,\n",
       " 'moms': 961,\n",
       " 'pregnant': 962,\n",
       " 'went': 963,\n",
       " 'queen': 964,\n",
       " 'beer': 965,\n",
       " 'labor': 966,\n",
       " 'celebrate': 967,\n",
       " 'budget': 968,\n",
       " 'force': 969,\n",
       " 'feeling': 970,\n",
       " 'members': 971,\n",
       " 'park': 972,\n",
       " 'bathroom': 973,\n",
       " 'driver': 974,\n",
       " 'lets': 975,\n",
       " 'rape': 976,\n",
       " 'bank': 977,\n",
       " 'demand': 978,\n",
       " 'worse': 979,\n",
       " 'leads': 980,\n",
       " 'tour': 981,\n",
       " 'whether': 982,\n",
       " 'likely': 983,\n",
       " 'reveal': 984,\n",
       " 'energy': 985,\n",
       " 'fake': 986,\n",
       " 'church': 987,\n",
       " '25': 988,\n",
       " 'weight': 989,\n",
       " 'emotional': 990,\n",
       " 'train': 991,\n",
       " 'humans': 992,\n",
       " 'lgbtq': 993,\n",
       " 'asking': 994,\n",
       " 'older': 995,\n",
       " 'rest': 996,\n",
       " 'color': 997,\n",
       " 'gave': 998,\n",
       " 'walk': 999,\n",
       " 'christian': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VRiNX58Rb3oJ"
   },
   "source": [
    "### Set number of words\n",
    "- Since the above 0th index doesn't have a word, add 1 to the length of the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Dfwq6ou8ck2f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26597\n"
     ]
    }
   ],
   "source": [
    "num_words = len(tokenizer.word_index) + 1\n",
    "print(num_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bUF1TuQa8ux0"
   },
   "source": [
    "### Load Glove Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vq5AIfRtMeZh"
   },
   "source": [
    "Downloaded glove embedding file (glove.6B.200d.txt) from https://drive.google.com/drive/folders/1xUnF35naPGU63xwRDVGc-DkZ3M8V5mMk and placed in current working directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "prHSzdQUcZhm"
   },
   "source": [
    "### Create embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "elZ-T5aFGZmZ"
   },
   "outputs": [],
   "source": [
    "EMBEDDING_FILE = 'glove.6B.200d.txt'\n",
    "\n",
    "embeddings = {}\n",
    "for o in open(EMBEDDING_FILE, encoding=\"utf8\"):\n",
    "    word = o.split(\" \")[0]\n",
    "    # print(word)\n",
    "    embd = o.split(\" \")[1:]\n",
    "    embd = np.asarray(embd, dtype='float32')\n",
    "    # print(embd)\n",
    "    embeddings[word] = embd\n",
    "\n",
    "# create a weight matrix for words in training docs\n",
    "embedding_matrix = np.zeros((num_words, embedding_size))\n",
    "\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u7IbWuEX82Ra"
   },
   "source": [
    "### Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4tv168Gmc3PY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import LSTM, Embedding, Dense, Bidirectional, Input, Flatten, Dropout\n",
    "\n",
    "inputs = Input(shape=(maxlen,)) # Input layer\n",
    "model = Embedding(num_words, embedding_size, \n",
    "                  embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
    "                  trainable=False)(inputs)\n",
    "model = Bidirectional(LSTM(units=500, dropout=0.2, recurrent_dropout=0.2))(model) # Bi-directional LSTM layer\n",
    "model = Flatten()(model) # Flatten\n",
    "model = Dense(200, activation='relu')(model) # Dense layer 1\n",
    "model = Dropout(0.2)(model) # Dropout 1\n",
    "model = Dense(100, activation='relu')(model) # Dense layer 2\n",
    "model = Dropout(0.2)(model) # Dropout 2\n",
    "model = Dense(50, activation='relu')(model) # Dense layer 3\n",
    "model = Dropout(0.2)(model) # Dropout 3\n",
    "out = Dense(1, activation='sigmoid')(model) # Sigmoid output layer\n",
    "\n",
    "model = Model(inputs, out) # Complete model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xoI7_8Y1cqTj"
   },
   "source": [
    "### Compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-jJiPHeNoJ3U"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         [(None, 25)]              0         \n",
      "_________________________________________________________________\n",
      "embedding_5 (Embedding)      (None, 25, 200)           5319400   \n",
      "_________________________________________________________________\n",
      "bidirectional_5 (Bidirection (None, 1000)              2804000   \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 200)               200200    \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 100)               20100     \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 8,348,801\n",
      "Trainable params: 3,029,401\n",
      "Non-trainable params: 5,319,400\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"]) # Compile the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7s4nmqcecw3a"
   },
   "source": [
    "### Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NN789zNnJ5PL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "668/668 [==============================] - 107s 161ms/step - loss: 0.4925 - accuracy: 0.7636 - val_loss: 0.4155 - val_accuracy: 0.8055\n",
      "Epoch 2/5\n",
      "668/668 [==============================] - 107s 160ms/step - loss: 0.3657 - accuracy: 0.8372 - val_loss: 0.3476 - val_accuracy: 0.8444\n",
      "Epoch 3/5\n",
      "668/668 [==============================] - 107s 161ms/step - loss: 0.2995 - accuracy: 0.8723 - val_loss: 0.3271 - val_accuracy: 0.8538\n",
      "Epoch 4/5\n",
      "668/668 [==============================] - 107s 160ms/step - loss: 0.2421 - accuracy: 0.8983 - val_loss: 0.3341 - val_accuracy: 0.8519\n",
      "Epoch 5/5\n",
      "668/668 [==============================] - 107s 160ms/step - loss: 0.1927 - accuracy: 0.9214 - val_loss: 0.3760 - val_accuracy: 0.8590\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2b043bcf188>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_sequences_pad, y_train, batch_size=32, epochs=5, validation_data=(X_test_sequences_pad, y_test), verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Validation accuracy: 85.90%**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have successfully build a Sarcasm Detection model for news headlines achieving **86% accuracy**"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Questions - Project 2 - Sequential Models in NLP - Sarcasm Detection.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
