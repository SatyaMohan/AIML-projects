{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recognizing multi-digit numbers in photographs captured at street level is an important component of modern-day map making. A classic example of a corpus of such street level photographs is Googleâ€™s Street View imagery comprised of hundreds of millions of geo-located 360 degree panoramic images. The ability to automatically transcribe an address number from a geolocated patch of pixels and associate the transcribed number with a known street address helps pinpoint, with a high degree of accuracy, the\n",
    "location of the building it represents.  \n",
    "<br>\n",
    "More broadly, recognizing numbers in photographs is a problem of interest to the optical character recognition community. While OCR on constrained domains like document processing is well studied, arbitrary multi-character text recognition in photographs is still highly challenging. This difficulty arises due to the wide variability in the visual appearance of text in the wild on account of a large range of fonts, colours, styles, orientations, and character arrangements. The recognition problem is further complicated by environmental factors such as lighting, shadows, secularities, and occlusions as well as by image acquisition factors such as resolution, motion, and focus blurs.  \n",
    "<br>\n",
    "In this project we will use dataset with images centred around a single digit (many of the images do contain some distractors at the sides). Although we are taking a sample of the data which is simpler, it is more complex than MNIST because of the distractors.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Description:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The Street View House Numbers (SVHN) Dataset**  \n",
    "<br>\n",
    "SVHN is a real-world image dataset for developing machine learning and object recognition algorithms with minimal requirement on data formatting but comes from a significantly harder, unsolved, real world problem (recognizing digits and numbers in natural scene images). SVHN is obtained from house numbers in Google Street View images.  \n",
    "Download from here: https://drive.google.com/file/d/1L2-WXzguhUsCArrFUc8EEkXcj33pahoS/view?usp=sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective of the project is to learn how to implement a simple image classification pipeline based on a deep neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "from tensorflow import keras\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the file, load train, test sets\n",
    "h5f = h5py.File('SVHN_single_grey1.h5', 'r')\n",
    "\n",
    "X_train, y_train = h5f['X_train'][:], h5f['y_train'][:]\n",
    "X_test, y_test = h5f['X_test'][:], h5f['y_test'][:]\n",
    "\n",
    "h5f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of image samples in Train set: 42000\n",
      "No. of image samples in Test set: 18000\n",
      "Shape of X_train: (42000, 32, 32)\n",
      "Shape of y_train: (42000,)\n",
      "Shape of X_test: (18000, 32, 32)\n",
      "Shape of y_test: (18000,)\n"
     ]
    }
   ],
   "source": [
    "# Examine the train and test sets\n",
    "\n",
    "print(f\"No. of image samples in Train set: {len(X_train)}\")\n",
    "print(f\"No. of image samples in Test set: {len(X_test)}\")\n",
    "\n",
    "print(f\"Shape of X_train: {X_train.shape}\")\n",
    "print(f\"Shape of y_train: {y_train.shape}\")\n",
    "print(f\"Shape of X_test: {X_test.shape}\")\n",
    "print(f\"Shape of y_test: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 6\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the images\n",
    "\n",
    "plt.imshow(X_train[1], cmap='gray')\n",
    "print(f\"Label: {y_train[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We one-hot encode our class labels as our NN model will use softmax function to give class probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label value before encoding: 6\n",
      "Label value after encoding: [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "print(f\"Label value before encoding: {y_train[1]}\")\n",
    "\n",
    "# We have labels from 0-9, hence num_classes=10\n",
    "y_train = to_categorical(y_train, num_classes=10)\n",
    "y_test = to_categorical(y_test, num_classes=10)\n",
    "\n",
    "print(f\"Label value after encoding: {y_train[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model structure:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We will use a sequential model with following layers (in order):  \n",
    "  * **Input layer:** We will reshape our input from 32x32 (2D) to 1024 (1D). Hence, our input layer will have **1024** neurons  \n",
    "  * **Batch normalization layer:** This layer normalizes the value at each image pixel wrt a particular batch. Hence in a batch, the values will be normally distributed  \n",
    "  * **Hidden layer 1:** This is our 1st hidden layer with 'ReLU' activation. It will have **400** neurons  \n",
    "  * **Hidden layer 2:** This is our 2nd hidden layer with 'ReLU' activation. It will have **250** neurons\n",
    "  * **Dropout layer 1:** This is our 1st dropout layer which is introduced to reduce overfitting and bring in some regularization. We will start with dropout rate of **0.5**  \n",
    "  * **Hidden layer 3:** This is our 3rd hidden layer with 'ReLU' activation. It will have **150** neurons  \n",
    "  * **Hidden layer 4:** This is our 4th hidden layer with 'ReLU' activation. It will have **100** neurons  \n",
    "  * **Dropout layer 2:** This is our 2nd dropout layer which is introduced to further reduce overfitting and bring in some regularization. We will use dropout rate of **0.5**\n",
    "  * **Hidden layer 5:** This is our 5th hidden layer with 'ReLU' activation. It will have **60** neurons  \n",
    "  * **Hidden layer 6:** This is our 6th hidden layer with 'ReLU' activation. It will have **30** neurons  \n",
    "  * **Dropout layer 3:** This is our 3rd dropout layer which is introduced to further reduce overfitting and bring in some regularization. We will use dropout rate of **0.3**\n",
    "  * **Output layer:** This is our output layer which will have **10** neurons (one each for a class label) and will have **softmax** activation which will give us class probabilities \n",
    "\n",
    "We will use **adam** optimizer with **Cross Entropy** for loss function. We will use **Accuracy** to measure model performance  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix the seed for random number generator\n",
    "import random\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Sequential model\n",
    "nn_model = keras.models.Sequential(name='ANN')\n",
    "\n",
    "# Input layer: Reshape data from 2D to 1D -> 32x32 to 1024\n",
    "nn_model.add(keras.layers.Reshape((1024,),input_shape=(32,32,), name='Input_Layer'))\n",
    "\n",
    "# Batch normalization layer\n",
    "nn_model.add(keras.layers.BatchNormalization(name='Batch_Normalization_Layer'))\n",
    "\n",
    "# Hidden layers 1 and 2\n",
    "nn_model.add(keras.layers.Dense(400, activation='relu', name='Hidden_Layer_1'))\n",
    "nn_model.add(keras.layers.Dense(250, activation='relu', name='Hidden_Layer_2'))\n",
    "\n",
    "# Dropout layer 1\n",
    "nn_model.add(keras.layers.Dropout(0.5, name='Dropout_Layer_1'))\n",
    "\n",
    "# Hidden layers 3 and 4\n",
    "nn_model.add(keras.layers.Dense(150, activation='relu', name='Hidden_Layer_3'))\n",
    "nn_model.add(keras.layers.Dense(100, activation='relu', name='Hidden_Layer_4'))\n",
    "\n",
    "# Dropout layer 2\n",
    "nn_model.add(keras.layers.Dropout(0.5, name='Dropout_Layer_2'))\n",
    "\n",
    "# Hidden layers 5 and 6\n",
    "nn_model.add(keras.layers.Dense(60, activation='relu', name='Hidden_Layer_5'))\n",
    "nn_model.add(keras.layers.Dense(30, activation='relu', name='Hidden_Layer_6'))\n",
    "\n",
    "# Dropout layer 3\n",
    "nn_model.add(keras.layers.Dropout(0.3, name='Dropout_Layer_3'))\n",
    "\n",
    "#Output layer\n",
    "nn_model.add(keras.layers.Dense(10, activation='softmax', name='Output_Layer'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify optimizer, loss function and evaluation metrics for the model\n",
    "nn_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"ANN\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input_Layer (Reshape)        (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "Batch_Normalization_Layer (B (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_1 (Dense)       (None, 400)               410000    \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_2 (Dense)       (None, 250)               100250    \n",
      "_________________________________________________________________\n",
      "Dropout_Layer_1 (Dropout)    (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_3 (Dense)       (None, 150)               37650     \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_4 (Dense)       (None, 100)               15100     \n",
      "_________________________________________________________________\n",
      "Dropout_Layer_2 (Dropout)    (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_5 (Dense)       (None, 60)                6060      \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_6 (Dense)       (None, 30)                1830      \n",
      "_________________________________________________________________\n",
      "Dropout_Layer_3 (Dropout)    (None, 30)                0         \n",
      "_________________________________________________________________\n",
      "Output_Layer (Dense)         (None, 10)                310       \n",
      "=================================================================\n",
      "Total params: 575,296\n",
      "Trainable params: 573,248\n",
      "Non-trainable params: 2,048\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Print the model Summary\n",
    "nn_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will now train the model for 100 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "84/84 [==============================] - 6s 71ms/step - loss: 2.3186 - accuracy: 0.1080 - val_loss: 2.2938 - val_accuracy: 0.1277\n",
      "Epoch 2/100\n",
      "84/84 [==============================] - 6s 71ms/step - loss: 2.2612 - accuracy: 0.1327 - val_loss: 2.1373 - val_accuracy: 0.2226\n",
      "Epoch 3/100\n",
      "84/84 [==============================] - 6s 69ms/step - loss: 2.0570 - accuracy: 0.2301 - val_loss: 1.7178 - val_accuracy: 0.3723\n",
      "Epoch 4/100\n",
      "84/84 [==============================] - 6s 69ms/step - loss: 1.7384 - accuracy: 0.3614 - val_loss: 1.4145 - val_accuracy: 0.5186\n",
      "Epoch 5/100\n",
      "84/84 [==============================] - 6s 69ms/step - loss: 1.5347 - accuracy: 0.4433 - val_loss: 1.2777 - val_accuracy: 0.5698\n",
      "Epoch 6/100\n",
      "84/84 [==============================] - 6s 69ms/step - loss: 1.3910 - accuracy: 0.5170 - val_loss: 1.1169 - val_accuracy: 0.6481\n",
      "Epoch 7/100\n",
      "84/84 [==============================] - 6s 70ms/step - loss: 1.2606 - accuracy: 0.5788 - val_loss: 0.9817 - val_accuracy: 0.6976\n",
      "Epoch 8/100\n",
      "84/84 [==============================] - 6s 70ms/step - loss: 1.1446 - accuracy: 0.6304 - val_loss: 0.9057 - val_accuracy: 0.7258\n",
      "Epoch 9/100\n",
      "84/84 [==============================] - 6s 70ms/step - loss: 1.0629 - accuracy: 0.6673 - val_loss: 0.8669 - val_accuracy: 0.7428\n",
      "Epoch 10/100\n",
      "84/84 [==============================] - 6s 70ms/step - loss: 1.0031 - accuracy: 0.6927 - val_loss: 0.8217 - val_accuracy: 0.7556\n",
      "Epoch 11/100\n",
      "84/84 [==============================] - 6s 70ms/step - loss: 0.9481 - accuracy: 0.7141 - val_loss: 0.7790 - val_accuracy: 0.7649\n",
      "Epoch 12/100\n",
      "84/84 [==============================] - 6s 71ms/step - loss: 0.8964 - accuracy: 0.7295 - val_loss: 0.7490 - val_accuracy: 0.7789\n",
      "Epoch 13/100\n",
      "84/84 [==============================] - 6s 69ms/step - loss: 0.8521 - accuracy: 0.7479 - val_loss: 0.7220 - val_accuracy: 0.7913\n",
      "Epoch 14/100\n",
      "84/84 [==============================] - 6s 70ms/step - loss: 0.8205 - accuracy: 0.7561 - val_loss: 0.7089 - val_accuracy: 0.7923\n",
      "Epoch 15/100\n",
      "84/84 [==============================] - 6s 70ms/step - loss: 0.7813 - accuracy: 0.7693 - val_loss: 0.6833 - val_accuracy: 0.8044\n",
      "Epoch 16/100\n",
      "84/84 [==============================] - 6s 70ms/step - loss: 0.7503 - accuracy: 0.7804 - val_loss: 0.6794 - val_accuracy: 0.8001\n",
      "Epoch 17/100\n",
      "84/84 [==============================] - 6s 70ms/step - loss: 0.7270 - accuracy: 0.7891 - val_loss: 0.6645 - val_accuracy: 0.8076\n",
      "Epoch 18/100\n",
      "84/84 [==============================] - 6s 70ms/step - loss: 0.7108 - accuracy: 0.7924 - val_loss: 0.6644 - val_accuracy: 0.8080\n",
      "Epoch 19/100\n",
      "84/84 [==============================] - 6s 70ms/step - loss: 0.6960 - accuracy: 0.7960 - val_loss: 0.6611 - val_accuracy: 0.8091\n",
      "Epoch 20/100\n",
      "84/84 [==============================] - 6s 70ms/step - loss: 0.6576 - accuracy: 0.8092 - val_loss: 0.6240 - val_accuracy: 0.8219\n",
      "Epoch 21/100\n",
      "84/84 [==============================] - 6s 70ms/step - loss: 0.6575 - accuracy: 0.8095 - val_loss: 0.6176 - val_accuracy: 0.8238\n",
      "Epoch 22/100\n",
      "84/84 [==============================] - 6s 71ms/step - loss: 0.6403 - accuracy: 0.8171 - val_loss: 0.6173 - val_accuracy: 0.8256\n",
      "Epoch 23/100\n",
      "84/84 [==============================] - 6s 70ms/step - loss: 0.6310 - accuracy: 0.8178 - val_loss: 0.6177 - val_accuracy: 0.8214\n",
      "Epoch 24/100\n",
      "84/84 [==============================] - 6s 71ms/step - loss: 0.6095 - accuracy: 0.8253 - val_loss: 0.6173 - val_accuracy: 0.8277\n",
      "Epoch 25/100\n",
      "84/84 [==============================] - 6s 71ms/step - loss: 0.5881 - accuracy: 0.8321 - val_loss: 0.6033 - val_accuracy: 0.8328\n",
      "Epoch 26/100\n",
      "84/84 [==============================] - 6s 71ms/step - loss: 0.5739 - accuracy: 0.8366 - val_loss: 0.5883 - val_accuracy: 0.8341\n",
      "Epoch 27/100\n",
      "84/84 [==============================] - 6s 70ms/step - loss: 0.5725 - accuracy: 0.8346 - val_loss: 0.5969 - val_accuracy: 0.8325\n",
      "Epoch 28/100\n",
      "84/84 [==============================] - 6s 70ms/step - loss: 0.5711 - accuracy: 0.8349 - val_loss: 0.6000 - val_accuracy: 0.8352\n",
      "Epoch 29/100\n",
      "84/84 [==============================] - 6s 71ms/step - loss: 0.5428 - accuracy: 0.8448 - val_loss: 0.5769 - val_accuracy: 0.8408\n",
      "Epoch 30/100\n",
      "84/84 [==============================] - 6s 70ms/step - loss: 0.5285 - accuracy: 0.8492 - val_loss: 0.5717 - val_accuracy: 0.8399\n",
      "Epoch 31/100\n",
      "84/84 [==============================] - 6s 70ms/step - loss: 0.5331 - accuracy: 0.8471 - val_loss: 0.5711 - val_accuracy: 0.8430\n",
      "Epoch 32/100\n",
      "84/84 [==============================] - 6s 73ms/step - loss: 0.5195 - accuracy: 0.8502 - val_loss: 0.5785 - val_accuracy: 0.8411\n",
      "Epoch 33/100\n",
      "84/84 [==============================] - 6s 71ms/step - loss: 0.5160 - accuracy: 0.8510 - val_loss: 0.5628 - val_accuracy: 0.8443\n",
      "Epoch 34/100\n",
      "84/84 [==============================] - 6s 74ms/step - loss: 0.4976 - accuracy: 0.8578 - val_loss: 0.5804 - val_accuracy: 0.8413\n",
      "Epoch 35/100\n",
      "84/84 [==============================] - 6s 71ms/step - loss: 0.5020 - accuracy: 0.8559 - val_loss: 0.5619 - val_accuracy: 0.8488\n",
      "Epoch 36/100\n",
      "84/84 [==============================] - 6s 74ms/step - loss: 0.4859 - accuracy: 0.8615 - val_loss: 0.5550 - val_accuracy: 0.8459\n",
      "Epoch 37/100\n",
      "84/84 [==============================] - 6s 75ms/step - loss: 0.4728 - accuracy: 0.8645 - val_loss: 0.5696 - val_accuracy: 0.8450\n",
      "Epoch 38/100\n",
      "84/84 [==============================] - 6s 71ms/step - loss: 0.4712 - accuracy: 0.8656 - val_loss: 0.5603 - val_accuracy: 0.8477\n",
      "Epoch 39/100\n",
      "84/84 [==============================] - 6s 71ms/step - loss: 0.4783 - accuracy: 0.8636 - val_loss: 0.5496 - val_accuracy: 0.8466\n",
      "Epoch 40/100\n",
      "84/84 [==============================] - 6s 71ms/step - loss: 0.4602 - accuracy: 0.8697 - val_loss: 0.5561 - val_accuracy: 0.8498\n",
      "Epoch 41/100\n",
      "84/84 [==============================] - 6s 70ms/step - loss: 0.4719 - accuracy: 0.8664 - val_loss: 0.5599 - val_accuracy: 0.8477\n",
      "Epoch 42/100\n",
      "84/84 [==============================] - 6s 71ms/step - loss: 0.4414 - accuracy: 0.8737 - val_loss: 0.5582 - val_accuracy: 0.8526\n",
      "Epoch 43/100\n",
      "84/84 [==============================] - 6s 72ms/step - loss: 0.4390 - accuracy: 0.8749 - val_loss: 0.5544 - val_accuracy: 0.8504\n",
      "Epoch 44/100\n",
      "84/84 [==============================] - 6s 71ms/step - loss: 0.4449 - accuracy: 0.8720 - val_loss: 0.5550 - val_accuracy: 0.8484\n",
      "Epoch 45/100\n",
      "84/84 [==============================] - 6s 72ms/step - loss: 0.4352 - accuracy: 0.8742 - val_loss: 0.5694 - val_accuracy: 0.8513\n",
      "Epoch 46/100\n",
      "84/84 [==============================] - 6s 72ms/step - loss: 0.4245 - accuracy: 0.8775 - val_loss: 0.5552 - val_accuracy: 0.8529\n",
      "Epoch 47/100\n",
      "84/84 [==============================] - 6s 72ms/step - loss: 0.4192 - accuracy: 0.8794 - val_loss: 0.5533 - val_accuracy: 0.8494\n",
      "Epoch 48/100\n",
      "84/84 [==============================] - 6s 73ms/step - loss: 0.4192 - accuracy: 0.8801 - val_loss: 0.5335 - val_accuracy: 0.8573\n",
      "Epoch 49/100\n",
      "84/84 [==============================] - 6s 73ms/step - loss: 0.4147 - accuracy: 0.8817 - val_loss: 0.5557 - val_accuracy: 0.8514\n",
      "Epoch 50/100\n",
      "84/84 [==============================] - 6s 72ms/step - loss: 0.4188 - accuracy: 0.8807 - val_loss: 0.5608 - val_accuracy: 0.8491\n",
      "Epoch 51/100\n",
      "84/84 [==============================] - 6s 73ms/step - loss: 0.4112 - accuracy: 0.8817 - val_loss: 0.5509 - val_accuracy: 0.8523\n",
      "Epoch 52/100\n",
      "84/84 [==============================] - 6s 73ms/step - loss: 0.4066 - accuracy: 0.8857 - val_loss: 0.5427 - val_accuracy: 0.8541\n",
      "Epoch 53/100\n",
      "84/84 [==============================] - 6s 73ms/step - loss: 0.3993 - accuracy: 0.8859 - val_loss: 0.5454 - val_accuracy: 0.8570\n",
      "Epoch 54/100\n",
      "84/84 [==============================] - 6s 72ms/step - loss: 0.3841 - accuracy: 0.8907 - val_loss: 0.5623 - val_accuracy: 0.8535\n",
      "Epoch 55/100\n",
      "84/84 [==============================] - 6s 72ms/step - loss: 0.3871 - accuracy: 0.8903 - val_loss: 0.5416 - val_accuracy: 0.8584\n",
      "Epoch 56/100\n",
      "84/84 [==============================] - 6s 73ms/step - loss: 0.3792 - accuracy: 0.8919 - val_loss: 0.5583 - val_accuracy: 0.8556\n",
      "Epoch 57/100\n",
      "84/84 [==============================] - 6s 72ms/step - loss: 0.3781 - accuracy: 0.8935 - val_loss: 0.5549 - val_accuracy: 0.8582\n",
      "Epoch 58/100\n",
      "84/84 [==============================] - 6s 70ms/step - loss: 0.3885 - accuracy: 0.8899 - val_loss: 0.5460 - val_accuracy: 0.8594\n",
      "Epoch 59/100\n",
      "84/84 [==============================] - 6s 71ms/step - loss: 0.3759 - accuracy: 0.8924 - val_loss: 0.5493 - val_accuracy: 0.8583\n",
      "Epoch 60/100\n",
      "84/84 [==============================] - 6s 72ms/step - loss: 0.3809 - accuracy: 0.8904 - val_loss: 0.5427 - val_accuracy: 0.8566\n",
      "Epoch 61/100\n",
      "84/84 [==============================] - 6s 72ms/step - loss: 0.3697 - accuracy: 0.8940 - val_loss: 0.5397 - val_accuracy: 0.8614\n",
      "Epoch 62/100\n",
      "84/84 [==============================] - 6s 71ms/step - loss: 0.3599 - accuracy: 0.8969 - val_loss: 0.5514 - val_accuracy: 0.8622\n",
      "Epoch 63/100\n",
      "84/84 [==============================] - 6s 72ms/step - loss: 0.3519 - accuracy: 0.8988 - val_loss: 0.5421 - val_accuracy: 0.8614\n",
      "Epoch 64/100\n",
      "84/84 [==============================] - 6s 72ms/step - loss: 0.3531 - accuracy: 0.8983 - val_loss: 0.5641 - val_accuracy: 0.8594\n",
      "Epoch 65/100\n",
      "84/84 [==============================] - 6s 72ms/step - loss: 0.3521 - accuracy: 0.8998 - val_loss: 0.5551 - val_accuracy: 0.86130.3525 - accu\n",
      "Epoch 66/100\n",
      "84/84 [==============================] - 6s 72ms/step - loss: 0.3559 - accuracy: 0.8978 - val_loss: 0.5448 - val_accuracy: 0.8606\n",
      "Epoch 67/100\n",
      "84/84 [==============================] - 6s 72ms/step - loss: 0.3425 - accuracy: 0.9025 - val_loss: 0.5673 - val_accuracy: 0.8574\n",
      "Epoch 68/100\n",
      "84/84 [==============================] - 6s 73ms/step - loss: 0.3397 - accuracy: 0.9024 - val_loss: 0.5518 - val_accuracy: 0.8634\n",
      "Epoch 69/100\n",
      "84/84 [==============================] - 6s 72ms/step - loss: 0.3405 - accuracy: 0.9019 - val_loss: 0.5577 - val_accuracy: 0.8602\n",
      "Epoch 70/100\n",
      "84/84 [==============================] - 6s 72ms/step - loss: 0.3432 - accuracy: 0.9037 - val_loss: 0.5516 - val_accuracy: 0.8609\n",
      "Epoch 71/100\n",
      "84/84 [==============================] - 6s 72ms/step - loss: 0.3362 - accuracy: 0.9040 - val_loss: 0.5482 - val_accuracy: 0.8606\n",
      "Epoch 72/100\n",
      "84/84 [==============================] - 6s 72ms/step - loss: 0.3254 - accuracy: 0.9068 - val_loss: 0.5596 - val_accuracy: 0.8610\n",
      "Epoch 73/100\n",
      "84/84 [==============================] - 6s 72ms/step - loss: 0.3355 - accuracy: 0.9050 - val_loss: 0.5599 - val_accuracy: 0.8606\n",
      "Epoch 74/100\n",
      "84/84 [==============================] - 6s 72ms/step - loss: 0.3300 - accuracy: 0.9048 - val_loss: 0.5500 - val_accuracy: 0.8617\n",
      "Epoch 75/100\n",
      "84/84 [==============================] - 6s 72ms/step - loss: 0.3197 - accuracy: 0.9081 - val_loss: 0.5698 - val_accuracy: 0.8595\n",
      "Epoch 76/100\n",
      "84/84 [==============================] - 6s 72ms/step - loss: 0.3281 - accuracy: 0.9050 - val_loss: 0.5766 - val_accuracy: 0.8597\n",
      "Epoch 77/100\n",
      "84/84 [==============================] - 6s 72ms/step - loss: 0.3330 - accuracy: 0.9051 - val_loss: 0.5627 - val_accuracy: 0.8628\n",
      "Epoch 78/100\n",
      "84/84 [==============================] - 6s 72ms/step - loss: 0.3223 - accuracy: 0.9078 - val_loss: 0.5564 - val_accuracy: 0.8607\n",
      "Epoch 79/100\n",
      "84/84 [==============================] - 6s 72ms/step - loss: 0.3243 - accuracy: 0.9075 - val_loss: 0.5726 - val_accuracy: 0.8607\n",
      "Epoch 80/100\n",
      "84/84 [==============================] - 6s 71ms/step - loss: 0.3227 - accuracy: 0.9082 - val_loss: 0.5591 - val_accuracy: 0.8668\n",
      "Epoch 81/100\n",
      "84/84 [==============================] - 6s 72ms/step - loss: 0.3153 - accuracy: 0.9089 - val_loss: 0.5602 - val_accuracy: 0.8639\n",
      "Epoch 82/100\n",
      "84/84 [==============================] - 6s 73ms/step - loss: 0.3123 - accuracy: 0.9101 - val_loss: 0.5649 - val_accuracy: 0.8651\n",
      "Epoch 83/100\n",
      "84/84 [==============================] - 6s 72ms/step - loss: 0.3023 - accuracy: 0.9143 - val_loss: 0.5560 - val_accuracy: 0.8641\n",
      "Epoch 84/100\n",
      "84/84 [==============================] - 6s 72ms/step - loss: 0.3039 - accuracy: 0.9130 - val_loss: 0.5651 - val_accuracy: 0.8606\n",
      "Epoch 85/100\n",
      "84/84 [==============================] - 6s 72ms/step - loss: 0.2985 - accuracy: 0.9139 - val_loss: 0.5501 - val_accuracy: 0.8645\n",
      "Epoch 86/100\n",
      "84/84 [==============================] - 6s 72ms/step - loss: 0.3005 - accuracy: 0.9139 - val_loss: 0.5727 - val_accuracy: 0.8642\n",
      "Epoch 87/100\n",
      "84/84 [==============================] - 6s 72ms/step - loss: 0.2963 - accuracy: 0.9152 - val_loss: 0.5720 - val_accuracy: 0.8658\n",
      "Epoch 88/100\n",
      "84/84 [==============================] - 6s 72ms/step - loss: 0.2983 - accuracy: 0.9147 - val_loss: 0.5843 - val_accuracy: 0.8601\n",
      "Epoch 89/100\n",
      "84/84 [==============================] - 6s 72ms/step - loss: 0.2948 - accuracy: 0.9143 - val_loss: 0.5591 - val_accuracy: 0.8651\n",
      "Epoch 90/100\n",
      "84/84 [==============================] - 6s 72ms/step - loss: 0.2888 - accuracy: 0.9182 - val_loss: 0.5925 - val_accuracy: 0.8616\n",
      "Epoch 91/100\n",
      "84/84 [==============================] - 6s 72ms/step - loss: 0.2947 - accuracy: 0.9153 - val_loss: 0.5481 - val_accuracy: 0.8671\n",
      "Epoch 92/100\n",
      "84/84 [==============================] - 6s 72ms/step - loss: 0.3001 - accuracy: 0.9154 - val_loss: 0.5512 - val_accuracy: 0.8639\n",
      "Epoch 93/100\n",
      "84/84 [==============================] - 6s 73ms/step - loss: 0.2871 - accuracy: 0.9163 - val_loss: 0.5556 - val_accuracy: 0.8676\n",
      "Epoch 94/100\n",
      "84/84 [==============================] - 6s 72ms/step - loss: 0.2828 - accuracy: 0.9175 - val_loss: 0.5774 - val_accuracy: 0.8672\n",
      "Epoch 95/100\n",
      "84/84 [==============================] - 6s 72ms/step - loss: 0.2857 - accuracy: 0.9183 - val_loss: 0.6040 - val_accuracy: 0.8558\n",
      "Epoch 96/100\n",
      "84/84 [==============================] - 6s 72ms/step - loss: 0.2895 - accuracy: 0.9161 - val_loss: 0.5714 - val_accuracy: 0.8664\n",
      "Epoch 97/100\n",
      "84/84 [==============================] - 6s 72ms/step - loss: 0.2900 - accuracy: 0.9170 - val_loss: 0.5720 - val_accuracy: 0.8620\n",
      "Epoch 98/100\n",
      "84/84 [==============================] - 6s 72ms/step - loss: 0.2703 - accuracy: 0.9223 - val_loss: 0.5651 - val_accuracy: 0.8691\n",
      "Epoch 99/100\n",
      "84/84 [==============================] - 6s 72ms/step - loss: 0.2744 - accuracy: 0.9211 - val_loss: 0.5710 - val_accuracy: 0.8651\n",
      "Epoch 100/100\n",
      "84/84 [==============================] - 6s 72ms/step - loss: 0.2700 - accuracy: 0.9227 - val_loss: 0.5879 - val_accuracy: 0.8648\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0xe954152d88>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100, batch_size = 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "563/563 [==============================] - 2s 3ms/step - loss: 0.5879 - accuracy: 0.8648\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5878511667251587, 0.8648333549499512]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check accuracy on test set\n",
    "nn_model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predicted probabilities and predicted class one-hot vectors\n",
    "\n",
    "y_pred_proba = nn_model.predict(X_test)\n",
    "y_pred_class = to_categorical(nn_model.predict_classes(X_test), num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.89      0.89      1814\n",
      "           1       0.84      0.89      0.87      1828\n",
      "           2       0.92      0.86      0.89      1803\n",
      "           3       0.80      0.83      0.81      1719\n",
      "           4       0.91      0.88      0.90      1812\n",
      "           5       0.83      0.88      0.85      1768\n",
      "           6       0.88      0.84      0.86      1832\n",
      "           7       0.91      0.89      0.90      1808\n",
      "           8       0.83      0.82      0.83      1812\n",
      "           9       0.86      0.85      0.85      1804\n",
      "\n",
      "   micro avg       0.86      0.86      0.86     18000\n",
      "   macro avg       0.87      0.86      0.86     18000\n",
      "weighted avg       0.87      0.86      0.87     18000\n",
      " samples avg       0.86      0.86      0.86     18000\n",
      "\n",
      "0.9842925786972175\n"
     ]
    }
   ],
   "source": [
    "# Print classification report\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "\n",
    "print(classification_report(y_test, y_pred_class))\n",
    "print(roc_auc_score(y_test, y_pred_proba))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model has ROC score of **0.98** and we are able to achieve an accuracy of **86.48%** on the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are successfully able to implement a simple image classification pipeline based on a deep neural network"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
